{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 1: Sentence Transformer Implementation**"
      ],
      "metadata": {
        "id": "eUaq0ajdn5Up"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "XwRTEVbp-h9w"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load a pre-trained Sentence Transformer model\n",
        "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "# Sample sentences to encode\n",
        "sentences = [\n",
        "    \"Machine learning is amazing!\",\n",
        "    \"Transformers are powerful for NLP tasks.\",\n",
        "    \"Sentence embeddings help capture semantic meaning.\"\n",
        "]\n",
        "\n",
        "# Encode sentences into fixed-length embeddings\n",
        "embeddings = model.encode(sentences)\n",
        "\n",
        "# Display embeddings\n",
        "for i, sentence in enumerate(sentences):\n",
        "    print(f\"Sentence: {sentence}\")\n",
        "    print(f\"Embedding: {embeddings[i][:5]}... (truncated)\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riSQzNn4oPn6",
        "outputId": "10a1e7cb-7542-429c-ba03-18c26358248d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: Machine learning is amazing!\n",
            "Embedding: [-0.46188468  0.04595638  1.1395051   0.20642886 -0.2947008 ]... (truncated)\n",
            "\n",
            "Sentence: Transformers are powerful for NLP tasks.\n",
            "Embedding: [ 0.2185293  -0.3930077   1.0106223   0.23320135 -0.76732814]... (truncated)\n",
            "\n",
            "Sentence: Sentence embeddings help capture semantic meaning.\n",
            "Embedding: [-0.3018907   0.24977122  0.8252767   0.5386457   0.0526836 ]... (truncated)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Architecture Choices**\n",
        "*  **Model Selection:** \"bert-base-nli-mean-tokens\" is chosen since it's pre-trained on Natural Language Inference (NLI) tasks, making it effective for sentence similarity and embeddings.\n",
        "*  **Framework:** The sentence-transformers library simplifies the implementation by providing efficient APIs for embedding generation.\n",
        "*  **Embedding Representation:** Each sentence is encoded into a fixed-length vector, suitable for downstream tasks like classification, clustering, and similarity scoring.\n",
        "*  **Performance Consideration:** Pre-trained models speed up development and avoid the need for training from scratch."
      ],
      "metadata": {
        "id": "UyKNNsN2xhwi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 2: Multi-Task Learning Expansion**"
      ],
      "metadata": {
        "id": "D24rEojotU4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "class MultiTaskSentenceTransformer(nn.Module):\n",
        "    def __init__(self, model_name=\"bert-base-nli-mean-tokens\", num_classes=3, num_sentiments=3):\n",
        "        super(MultiTaskSentenceTransformer, self).__init__()\n",
        "\n",
        "        # Load pre-trained transformer as the shared backbone\n",
        "        self.sentence_transformer = SentenceTransformer(model_name)\n",
        "\n",
        "        # Task A: Sentence Classification head\n",
        "        self.classification_head = nn.Linear(768, num_classes)\n",
        "\n",
        "        # Task B: Sentiment Analysis head\n",
        "        self.sentiment_head = nn.Linear(768, num_sentiments)\n",
        "\n",
        "    def forward(self, sentences):\n",
        "        # Encode sentences using the shared transformer\n",
        "        embeddings = self.sentence_transformer.encode(sentences, convert_to_tensor=True)\n",
        "\n",
        "        # Task A: Sentence Classification\n",
        "        class_logits = self.classification_head(embeddings)\n",
        "\n",
        "        # Task B: Sentiment Analysis\n",
        "        sentiment_logits = self.sentiment_head(embeddings)\n",
        "\n",
        "        return class_logits, sentiment_logits\n",
        "\n",
        "\n",
        "# Instantiate the model\n",
        "model = MultiTaskSentenceTransformer()\n",
        "\n",
        "# Sample input sentences\n",
        "sample_sentences = [\"I love AI research!\", \"The movie was terrible.\", \"This is a great innovation.\"]\n",
        "\n",
        "# Forward pass\n",
        "class_outputs, sentiment_outputs = model(sample_sentences)\n",
        "\n",
        "print(\"Task A - Classification Output:\", class_outputs)\n",
        "print(\"Task B - Sentiment Output:\", sentiment_outputs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mziDTYvtfju",
        "outputId": "f866b482-02c3-43d0-ab9f-f3d0f1a82085"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task A - Classification Output: tensor([[ 0.2454,  0.0176,  0.3753],\n",
            "        [-0.1844,  0.1087,  0.5830],\n",
            "        [ 0.3544, -0.0117,  0.8064]], grad_fn=<AddmmBackward0>)\n",
            "Task B - Sentiment Output: tensor([[ 0.3242, -0.1185,  0.3239],\n",
            "        [ 0.4710,  0.2860, -0.0037],\n",
            "        [-0.0236,  0.2406,  0.4187]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " #### **Changes made to the architecture**"
      ],
      "metadata": {
        "id": "X6tfE7sA0bQM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Shared Encoder:** Retain the pre-trained Sentence Transformer as a common encoder to generate embeddings for all tasks.\n",
        "\n",
        "**Task-Specific Output Heads:** Introduce separate output layers for each task, such as:\n",
        "* A linear layer for sentence classification.\n",
        "* Another linear layer for sentiment analysis.\n",
        "\n",
        "**Modified Forward Pass:**\n",
        "\n",
        "* Process input sentences through the shared encoder to obtain embeddings.\n",
        "* Pass these embeddings through each task-specific output head to generate respective predictions.\n",
        "\n"
      ],
      "metadata": {
        "id": "dmI3na1t0lgK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 3: Training Considerations**"
      ],
      "metadata": {
        "id": "EagtDoKJuOmZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Scenario 1: Freezing the Entire Network**"
      ],
      "metadata": {
        "id": "Y1h0Cu822-2L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What it means:** All model parameters (transformer + task-specific heads) are frozen, meaning no learning occurs.\n",
        "\n",
        "**Implication:** This is only useful if using the model for inference without training.\n",
        "\n",
        "**When to use:**\n",
        "* When using the model for zero-shot learning without fine-tuning.\n",
        "* If the pre-trained embeddings are already sufficient for downstream tasks."
      ],
      "metadata": {
        "id": "FD-X6-Xr3YCd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Scenario 2: Freezing Only the Transformer Backbone**"
      ],
      "metadata": {
        "id": "TH6D-Nyf3E4V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What it means:** The transformer model remains frozen, but the task-specific heads (classification & sentiment) are trainable.\n",
        "\n",
        "**Implication:**\n",
        "* Helps preserve pre-trained knowledge.\n",
        "* educes computational cost since only a small part of the model updates.\n",
        "* Might limit performance if embeddings are not well-suited for the tasks.\n",
        "\n",
        "**When to use:**\n",
        "* If thereâ€™s limited data and we want to prevent catastrophic forgetting.\n",
        "* When fine-tuning for domain-specific tasks while keeping general language understanding intact.\n"
      ],
      "metadata": {
        "id": "m4zfeFvf36Ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Scenario 3: Freezing Only One Task-Specific Head**"
      ],
      "metadata": {
        "id": "uq4Vd4xs3OTa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**What it means:** One of the task-specific heads is frozen while the rest of the model trains.\n",
        "\n",
        "**Implication:**\n",
        "* Allows improving one task while keeping the learned knowledge of another intact.\n",
        "* Useful when one task is already well-trained and does not require further updates.\n",
        "\n",
        "**When to use:**\n",
        "* If one task head has converged to a good performance level.\n",
        "* To prevent a well-trained head from overfitting on noisy data."
      ],
      "metadata": {
        "id": "aXXi15JQ4Nzv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Transfer Learning Approach**"
      ],
      "metadata": {
        "id": "4aHldhSL4_WI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "**Choosing a Pre-Trained Model:**\n",
        "\n",
        "* Select a model like bert-base-nli-mean-tokens, roberta-base, or distilbert-base-uncased.\n",
        "* If domain-specific, consider models like BioBERT (for medical texts) or LegalBERT (for legal texts).\n",
        "\n",
        "**Freezing vs. Unfreezing Layers:**\n",
        "\n",
        "* Start by freezing the transformer and training only the task heads.\n",
        "* Gradually unfreeze transformer layers if performance does not improve.\n",
        "\n",
        "**Fine-Tuning Strategy:**\n",
        "\n",
        "* Use progressive unfreezing, where deeper layers are unfrozen gradually.\n",
        "* Apply differential learning rates, with lower rates for the transformer and higher for task heads."
      ],
      "metadata": {
        "id": "2Ux6Ti3t4ozL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Task 4: Training Loop Implementation (BONUS)**"
      ],
      "metadata": {
        "id": "nVpE26HWuhpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Define the Multi-Task Learning model\n",
        "class MultiTaskSentenceTransformer(nn.Module):\n",
        "    def __init__(self, model_name=\"bert-base-nli-mean-tokens\", num_classes=2, num_sentiments=3):\n",
        "        super(MultiTaskSentenceTransformer, self).__init__()\n",
        "        self.sentence_transformer = SentenceTransformer(model_name)\n",
        "        self.classification_head = nn.Linear(768, num_classes)\n",
        "        self.sentiment_head = nn.Linear(768, num_sentiments)\n",
        "\n",
        "    def forward(self, sentences):\n",
        "        embeddings = self.sentence_transformer.encode(sentences, convert_to_tensor=True)\n",
        "        class_logits = self.classification_head(embeddings)\n",
        "        sentiment_logits = self.sentiment_head(embeddings)\n",
        "        return class_logits, sentiment_logits\n",
        "\n",
        "# Custom Dataset class\n",
        "class MultiTaskDataset(Dataset):\n",
        "    def __init__(self, sentences, class_labels, sentiment_labels):\n",
        "        self.sentences = sentences\n",
        "        self.class_labels = class_labels\n",
        "        self.sentiment_labels = sentiment_labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.sentences[idx]\n",
        "        class_label = self.class_labels[idx]\n",
        "        sentiment_label = self.sentiment_labels[idx]\n",
        "        return sentence, class_label, sentiment_label\n",
        "\n",
        "# Sample data\n",
        "sentences = [\n",
        "    \"I love AI research!\",\n",
        "    \"The movie was terrible.\",\n",
        "    \"Deep learning is fascinating.\",\n",
        "    \"This restaurant is awful.\",\n",
        "    \"I enjoy studying machine learning.\"\n",
        "]\n",
        "class_labels = torch.tensor([0, 1, 0, 1, 0])  # Example category labels\n",
        "sentiment_labels = torch.tensor([1, 0, 2, 0, 1])  # Example sentiment labels 0 - negative, 1 - positive , 2 - neutral\n",
        "\n",
        "# Create Dataset and DataLoader\n",
        "dataset = MultiTaskDataset(sentences, class_labels, sentiment_labels)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = MultiTaskSentenceTransformer()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for batch_sentences, batch_class_labels, batch_sentiment_labels in dataloader:\n",
        "        class_outputs, sentiment_outputs = model(batch_sentences)\n",
        "        loss_a = criterion(class_outputs, batch_class_labels)\n",
        "        loss_b = criterion(sentiment_outputs, batch_sentiment_labels)\n",
        "        loss = loss_a + loss_b\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "print(\"Training complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBucDHahunyz",
        "outputId": "507a1970-3b9f-4f5f-98b5-7e71db98c580"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Loss: 4.6553\n",
            "Epoch 2/5, Loss: 2.2863\n",
            "Epoch 3/5, Loss: 1.2797\n",
            "Epoch 4/5, Loss: 0.9284\n",
            "Epoch 5/5, Loss: 0.6568\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Key Features of This Training Loop**\n",
        "**Data Handling:**\n",
        "\n",
        "* Uses a custom PyTorch Dataset to simulate multi-task learning data.\n",
        "* Outputs both classification labels and sentiment labels.\n",
        "\n",
        "**Forward Pass:**\n",
        "\n",
        "* Computes embeddings using SentenceTransformer.\n",
        "* Passes embeddings through two separate task-specific heads.\n",
        "\n",
        "**Loss Computation:**\n",
        "\n",
        "* Uses CrossEntropyLoss for both tasks.\n",
        "* Final loss = loss for Task A + loss for Task B (ensuring both tasks contribute to training).\n",
        "\n",
        "**Optimization:**\n",
        "\n",
        "* Adam optimizer updates only trainable parameters.\n",
        "* Uses mini-batches to improve efficiency.\n",
        "\n",
        "**Performance Metrics:**\n",
        "\n",
        "* Tracks total loss per epoch.\n",
        "* Can be extended with accuracy, precision, recall, etc."
      ],
      "metadata": {
        "id": "CHreY-bV6YZ7"
      }
    }
  ]
}