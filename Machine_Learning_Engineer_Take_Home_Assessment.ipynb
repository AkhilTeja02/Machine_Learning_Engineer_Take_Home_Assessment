{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 1: Sentence Transformer Implementation**"
      ],
      "metadata": {
        "id": "eUaq0ajdn5Up"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "XwRTEVbp-h9w"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load a pre-trained Sentence Transformer model\n",
        "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "# Sample sentences to encode\n",
        "sentences = [\n",
        "    \"Machine learning is amazing!\",\n",
        "    \"Transformers are powerful for NLP tasks.\",\n",
        "    \"Sentence embeddings help capture semantic meaning.\"\n",
        "]\n",
        "\n",
        "# Encode sentences into fixed-length embeddings\n",
        "embeddings = model.encode(sentences)\n",
        "\n",
        "# Display embeddings\n",
        "for i, sentence in enumerate(sentences):\n",
        "    print(f\"Sentence: {sentence}\")\n",
        "    print(f\"Embedding: {embeddings[i][:5]}... (truncated)\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riSQzNn4oPn6",
        "outputId": "3289de99-ca46-4677-c864-2e07649594e8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: Machine learning is amazing!\n",
            "Embedding: [-0.46188468  0.04595638  1.1395051   0.20642886 -0.2947008 ]... (truncated)\n",
            "\n",
            "Sentence: Transformers are powerful for NLP tasks.\n",
            "Embedding: [ 0.2185293  -0.3930077   1.0106223   0.23320135 -0.76732814]... (truncated)\n",
            "\n",
            "Sentence: Sentence embeddings help capture semantic meaning.\n",
            "Embedding: [-0.3018907   0.24977122  0.8252767   0.5386457   0.0526836 ]... (truncated)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Architecture Choices**\n",
        "*  **Model Selection:** \"bert-base-nli-mean-tokens\" is chosen since it's pre-trained on Natural Language Inference (NLI) tasks, making it effective for sentence similarity and embeddings.\n",
        "*  **Framework:** The sentence-transformers library simplifies the implementation by providing efficient APIs for embedding generation.\n",
        "*  **Embedding Representation:** Each sentence is encoded into a fixed-length vector, suitable for downstream tasks like classification, clustering, and similarity scoring.\n",
        "*  **Performance Consideration:** Pre-trained models speed up development and avoid the need for training from scratch."
      ],
      "metadata": {
        "id": "UyKNNsN2xhwi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 2: Multi-Task Learning Expansion**"
      ],
      "metadata": {
        "id": "D24rEojotU4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "class MultiTaskSentenceTransformer(nn.Module):\n",
        "    def __init__(self, model_name=\"bert-base-nli-mean-tokens\", num_classes=3, num_sentiments=3):\n",
        "        \"\"\"\n",
        "        Initializes the MultiTaskSentenceTransformer model.\n",
        "\n",
        "        Parameters:\n",
        "        - model_name (str): Pre-trained SentenceTransformer model name.\n",
        "        - num_classes (int): Number of classes for sentence classification.\n",
        "        - num_sentiments (int): Number of sentiment categories.\n",
        "        \"\"\"\n",
        "        super(MultiTaskSentenceTransformer, self).__init__()\n",
        "\n",
        "        # Shared Transformer Backbone\n",
        "        self.sentence_transformer = SentenceTransformer(model_name)\n",
        "\n",
        "        # Task A: Sentence Classification Head\n",
        "        self.classification_head = nn.Linear(768, num_classes)\n",
        "\n",
        "        # Task B: Sentiment Analysis Head\n",
        "        self.sentiment_head = nn.Linear(768, num_sentiments)\n",
        "\n",
        "    def forward(self, sentences):\n",
        "        \"\"\"\n",
        "        Forward pass to compute logits for both tasks.\n",
        "\n",
        "        Parameters:\n",
        "        - sentences (list of str): Input sentences.\n",
        "\n",
        "        Returns:\n",
        "        - class_logits (Tensor): Logits for sentence classification.\n",
        "        - sentiment_logits (Tensor): Logits for sentiment analysis.\n",
        "        \"\"\"\n",
        "        # Encode sentences to obtain embeddings\n",
        "        embeddings = self.sentence_transformer.encode(sentences, convert_to_tensor=True)\n",
        "\n",
        "        # Task A: Sentence Classification\n",
        "        class_logits = self.classification_head(embeddings)\n",
        "\n",
        "        # Task B: Sentiment Analysis\n",
        "        sentiment_logits = self.sentiment_head(embeddings)\n",
        "\n",
        "        return class_logits, sentiment_logits\n",
        "\n",
        "# Sample Usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Instantiate the model\n",
        "    model = MultiTaskSentenceTransformer()\n",
        "\n",
        "    # Sample input sentences\n",
        "    sample_sentences = [\"I love AI research!\", \"The movie was terrible.\", \"This is a great innovation.\"]\n",
        "\n",
        "    # Forward pass\n",
        "    class_outputs, sentiment_outputs = model(sample_sentences)\n",
        "\n",
        "    # Display outputs\n",
        "    print(\"Task A - Classification Output:\", class_outputs)\n",
        "    print(\"Task B - Sentiment Output:\", sentiment_outputs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mziDTYvtfju",
        "outputId": "10cd7ab6-661f-4e8a-b242-9b842cdbf1ff"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task A - Classification Output: tensor([[ 0.2470,  0.0291, -0.0509],\n",
            "        [-0.1161, -0.0526, -0.1133],\n",
            "        [ 0.1699, -0.0923, -0.0923]], grad_fn=<AddmmBackward0>)\n",
            "Task B - Sentiment Output: tensor([[ 0.3182,  0.5103, -0.2492],\n",
            "        [-0.2556, -0.2189, -0.1773],\n",
            "        [-0.3505,  0.2606, -0.2985]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " #### **Changes made to the architecture**"
      ],
      "metadata": {
        "id": "X6tfE7sA0bQM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Shared Encoder:** Retain the pre-trained Sentence Transformer as a common encoder to generate embeddings for all tasks.\n",
        "\n",
        "**Task-Specific Output Heads:** Introduce separate output layers for each task, such as:\n",
        "* A linear layer for sentence classification.\n",
        "* Another linear layer for sentiment analysis.\n",
        "\n",
        "**Modified Forward Pass:**\n",
        "\n",
        "* Process input sentences through the shared encoder to obtain embeddings.\n",
        "* Pass these embeddings through each task-specific output head to generate respective predictions.\n",
        "\n"
      ],
      "metadata": {
        "id": "dmI3na1t0lgK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 3: Training Considerations**"
      ],
      "metadata": {
        "id": "EagtDoKJuOmZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Scenario 1: Freezing the Entire Network**"
      ],
      "metadata": {
        "id": "Y1h0Cu822-2L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What it means:** All model parameters (transformer + task-specific heads) are frozen, meaning no learning occurs.\n",
        "\n",
        "**Implication:** This is only useful if using the model for inference without training.\n",
        "\n",
        "**When to use:**\n",
        "* When using the model for zero-shot learning without fine-tuning.\n",
        "* If the pre-trained embeddings are already sufficient for downstream tasks."
      ],
      "metadata": {
        "id": "FD-X6-Xr3YCd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Scenario 2: Freezing Only the Transformer Backbone**"
      ],
      "metadata": {
        "id": "TH6D-Nyf3E4V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What it means:** The transformer model remains frozen, but the task-specific heads (classification & sentiment) are trainable.\n",
        "\n",
        "**Implication:**\n",
        "* Helps preserve pre-trained knowledge.\n",
        "* educes computational cost since only a small part of the model updates.\n",
        "* Might limit performance if embeddings are not well-suited for the tasks.\n",
        "\n",
        "**When to use:**\n",
        "* If there’s limited data and we want to prevent catastrophic forgetting.\n",
        "* When fine-tuning for domain-specific tasks while keeping general language understanding intact.\n"
      ],
      "metadata": {
        "id": "m4zfeFvf36Ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Scenario 3: Freezing Only One Task-Specific Head**"
      ],
      "metadata": {
        "id": "uq4Vd4xs3OTa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**What it means:** One of the task-specific heads is frozen while the rest of the model trains.\n",
        "\n",
        "**Implication:**\n",
        "* Allows improving one task while keeping the learned knowledge of another intact.\n",
        "* Useful when one task is already well-trained and does not require further updates.\n",
        "\n",
        "**When to use:**\n",
        "* If one task head has converged to a good performance level.\n",
        "* To prevent a well-trained head from overfitting on noisy data."
      ],
      "metadata": {
        "id": "aXXi15JQ4Nzv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Transfer Learning Approach**"
      ],
      "metadata": {
        "id": "4aHldhSL4_WI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "**Choosing a Pre-Trained Model:**\n",
        "\n",
        "* Select a model like bert-base-nli-mean-tokens, roberta-base, or distilbert-base-uncased.\n",
        "* If domain-specific, consider models like BioBERT (for medical texts) or LegalBERT (for legal texts).\n",
        "\n",
        "**Freezing vs. Unfreezing Layers:**\n",
        "\n",
        "* Start by freezing the transformer and training only the task heads.\n",
        "* Gradually unfreeze transformer layers if performance does not improve.\n",
        "\n",
        "**Fine-Tuning Strategy:**\n",
        "\n",
        "* Use progressive unfreezing, where deeper layers are unfrozen gradually.\n",
        "* Apply differential learning rates, with lower rates for the transformer and higher for task heads."
      ],
      "metadata": {
        "id": "2Ux6Ti3t4ozL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Task 4: Training Loop Implementation (BONUS)**"
      ],
      "metadata": {
        "id": "nVpE26HWuhpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "# Define Multi-Task Learning model with a shared Transformer backbone\n",
        "class MultiTaskSentenceTransformer(nn.Module):\n",
        "    def __init__(self, model_name=\"bert-base-uncased\", num_classes=2, num_sentiments=3):\n",
        "        super(MultiTaskSentenceTransformer, self).__init__()\n",
        "\n",
        "        # Shared Transformer Backbone\n",
        "        self.bert = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "        # Task-specific heads\n",
        "        hidden_size = self.bert.config.hidden_size\n",
        "        self.classification_head = nn.Linear(hidden_size, num_classes)\n",
        "        self.sentiment_head = nn.Linear(hidden_size, num_sentiments)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output  # Get [CLS] token representation\n",
        "\n",
        "        # Multi-task outputs\n",
        "        class_logits = self.classification_head(pooled_output)\n",
        "        sentiment_logits = self.sentiment_head(pooled_output)\n",
        "\n",
        "        return class_logits, sentiment_logits\n",
        "\n",
        "\n",
        "# Custom Dataset class with tokenization\n",
        "class MultiTaskDataset(Dataset):\n",
        "    def __init__(self, sentences, class_labels, sentiment_labels, tokenizer, max_length=128):\n",
        "        self.sentences = sentences\n",
        "        self.class_labels = torch.tensor(class_labels, dtype=torch.long)\n",
        "        self.sentiment_labels = torch.tensor(sentiment_labels, dtype=torch.long)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoded = self.tokenizer(\n",
        "            self.sentences[idx],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": encoded[\"input_ids\"].squeeze(0),  # Remove batch dim\n",
        "            \"attention_mask\": encoded[\"attention_mask\"].squeeze(0),\n",
        "            \"class_label\": self.class_labels[idx],\n",
        "            \"sentiment_label\": self.sentiment_labels[idx]\n",
        "        }\n",
        "\n",
        "\n",
        "# Load tokenizer and define data\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "sentences = [\n",
        "    \"I love AI research!\",\n",
        "    \"The movie was terrible.\",\n",
        "    \"Deep learning is fascinating.\",\n",
        "    \"This restaurant is awful.\",\n",
        "    \"I enjoy studying machine learning.\"\n",
        "]\n",
        "class_labels = [0, 1, 0, 1, 0]  # Example category labels\n",
        "sentiment_labels = [1, 0, 1, 0, 1]  # 0 = negative, 1 = positive, 2 = neutral\n",
        "\n",
        "# Create Dataset and DataLoader\n",
        "dataset = MultiTaskDataset(sentences, class_labels, sentiment_labels, tokenizer)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MultiTaskSentenceTransformer().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        class_labels = batch[\"class_label\"].to(device)\n",
        "        sentiment_labels = batch[\"sentiment_label\"].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        class_outputs, sentiment_outputs = model(input_ids, attention_mask)\n",
        "\n",
        "        # Compute loss\n",
        "        loss_class = criterion(class_outputs, class_labels)\n",
        "        loss_sentiment = criterion(sentiment_outputs, sentiment_labels)\n",
        "        loss = loss_class + loss_sentiment  # Multi-task loss\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "print(\"Training complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBucDHahunyz",
        "outputId": "b35e4fda-3730-4a35-ea7d-fcbf02fa2b15"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 5.5045\n",
            "Epoch 2/10, Loss: 5.1106\n",
            "Epoch 3/10, Loss: 4.8322\n",
            "Epoch 4/10, Loss: 4.3502\n",
            "Epoch 5/10, Loss: 3.6897\n",
            "Epoch 6/10, Loss: 3.2161\n",
            "Epoch 7/10, Loss: 2.7861\n",
            "Epoch 8/10, Loss: 2.3550\n",
            "Epoch 9/10, Loss: 2.2787\n",
            "Epoch 10/10, Loss: 1.8269\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Key Features of This Training Loop**\n",
        "**Data Handling:**\n",
        "\n",
        "* Uses a custom PyTorch Dataset to simulate multi-task learning data.\n",
        "* Outputs both classification labels and sentiment labels.\n",
        "\n",
        "**Forward Pass:**\n",
        "\n",
        "* Computes embeddings using SentenceTransformer.\n",
        "* Passes embeddings through two separate task-specific heads.\n",
        "\n",
        "**Loss Computation:**\n",
        "\n",
        "* Uses CrossEntropyLoss for both tasks.\n",
        "* Final loss = loss for Task A + loss for Task B (ensuring both tasks contribute to training).\n",
        "\n",
        "**Optimization:**\n",
        "\n",
        "* Adam optimizer updates only trainable parameters.\n",
        "* Uses mini-batches to improve efficiency.\n",
        "\n",
        "**Performance Metrics:**\n",
        "\n",
        "* Tracks total loss per epoch.\n",
        "* Can be extended with accuracy, precision, recall, etc."
      ],
      "metadata": {
        "id": "CHreY-bV6YZ7"
      }
    }
  ]
}